# ==============================================================================
# HN Herald Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your values.
# Required variables must be set for the application to start.
# Optional variables have sensible defaults.
# ==============================================================================

# ==============================================================================
# REQUIRED - API Keys
# ==============================================================================

# Anthropic API key for Claude LLM access
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# ==============================================================================
# APPLICATION SETTINGS (Optional)
# ==============================================================================
# All prefixed with HN_HERALD_ to avoid collisions with other applications

# Environment: development | production
# - development: enables debug logging, hot reload
# - production: optimized for performance, structured JSON logs
HN_HERALD_ENV=development

# Logging level: DEBUG | INFO | WARNING | ERROR | CRITICAL
# Use DEBUG during development, INFO or WARNING in production
HN_HERALD_LOG_LEVEL=INFO

# Server binding address
# Use 0.0.0.0 for Docker/production, 127.0.0.1 for local-only access
HN_HERALD_HOST=0.0.0.0

# Server port
HN_HERALD_PORT=8000

# ==============================================================================
# LLM SETTINGS (Optional)
# ==============================================================================
# Configuration for Claude API calls

# Claude model to use for summarization and scoring
# claude-sonnet-4-20250514 offers good balance of speed and quality
LLM_MODEL=claude-sonnet-4-20250514

# Temperature for LLM responses (0-1)
# 0 = deterministic, higher = more creative
# Use 0 for consistent, reproducible summaries
LLM_TEMPERATURE=0

# Maximum tokens per LLM response
# 4096 is sufficient for article summaries
LLM_MAX_TOKENS=4096

# ==============================================================================
# LANGSMITH OBSERVABILITY (Optional)
# ==============================================================================
# LangSmith provides tracing, debugging, and monitoring for LLM operations
# Sign up at: https://smith.langchain.com/

# Enable LangSmith tracing (true | false)
# Recommended for development and production monitoring
LANGCHAIN_TRACING_V2=true

# LangSmith API key
# Get your key at: https://smith.langchain.com/settings
LANGCHAIN_API_KEY=ls-...

# Project name for organizing traces in LangSmith dashboard
LANGCHAIN_PROJECT=hn-herald

# ==============================================================================
# CACHING SETTINGS (Optional)
# ==============================================================================
# LLM response caching reduces API costs and latency

# Cache type: sqlite | memory | none
# - sqlite: persistent across restarts, stored in .cache/llm_cache.db
# - memory: fast but session-scoped, cleared on restart
# - none: disable caching (useful for debugging)
LLM_CACHE_TYPE=sqlite

# Cache time-to-live in seconds
# 86400 = 24 hours (recommended for article summaries)
LLM_CACHE_TTL=86400

# ==============================================================================
# FETCHING SETTINGS (Optional)
# ==============================================================================
# Timeouts and limits for external API calls

# Timeout for HackerNews API requests in seconds
# HN API is generally fast; 30s handles slow responses
HN_API_TIMEOUT=30

# Timeout for fetching article content in seconds
# Some websites are slow; 15s balances coverage vs speed
ARTICLE_FETCH_TIMEOUT=15

# Maximum article content length in characters
# Content beyond this is truncated before summarization
# 8000 chars provides good context without exceeding token limits
MAX_CONTENT_LENGTH=8000

# ==============================================================================
# PERFORMANCE SETTINGS (Optional)
# ==============================================================================
# Concurrency and batching for optimal throughput

# Maximum concurrent article fetches
# Higher = faster but more memory/connections
# 10 is a good balance for most systems
MAX_CONCURRENT_FETCHES=10

# Number of articles to summarize in a single LLM batch
# Batching reduces API calls and improves throughput
# 5 provides good parallelism without overwhelming the API
SUMMARY_BATCH_SIZE=5
